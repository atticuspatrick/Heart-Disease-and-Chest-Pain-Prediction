---
title: "Predicting Heart Disease and Chest Pain Type"
author: "Cam Lunn, Atticus Patrick, & Owen Patrick"
date: "5/11/2022"
output:
  word_document: default
  html_document: default
  pdf_document:
    latex_engine: xelatex
---

## Abstract

We will be analyzing a public heart disease data set from kaggle where each row is an
individual patient. As of now, our aim is to look into the key factors that determine heart
disease and predict the occurrence of heart disease in individuals based on a number of
heart-health related predictor variables. A secondary goal is to look at chest pain type and to try and predict this in patients as well. The data used in this study consists of 5 independent sub-datasets of heart health related data. The main response variables
looked at in the study are heart disease status and chest pain type. We found that the data
are well suited to make predictions for heart disease status when using a decision tree.
Additionally, we discovered that predicting chest pain type was very difficult and could not fit
an accurate model using KNN, QDA, or a decision tree.

## Introduction

Every year, 25% of all deaths in the US are attributed to heart disease. There are many
different types, which respectively can have different root causes. Malfunctions of the valves,
arteries, and other physiological components can lead to a patient developing heart disease.
On the other hand, lack of exercise, diet, and other environmental and even genetic factors
can play a role in this outcome as well. To be succinct: heart disease is one of the biggest
health-related killer the United States faces. If we can better understand the variables that
comprise the complex system of developing heart disease, we have a better shot at
preventing it from happening.
The main goal for this study is to determine what factors are associated with heart disease,
and if they can be used to predict a patientâ€™s outcome for it, as well as what factors are
associated with chest pain, and which of these factors can be used to predict types of chest
pain.

Our goals / hypotheses:

1) Exploratory analysis: look at descriptive statistics, and group means. See if there are
any relationships between variables, and look at a correlation matrix of the numeric
variables.

2) Use PCA to see which variables are most important and related to each other.

3) See if heart disease and chest pain type can be classified:
  a) LDA/QDA
  b) KNN
  c) Decision Tree
  
4) See if factor analysis is applicable.
  
## Data Description

|  Name     |       Description                     |    Levels                   |
|:---------:|    :-----------------------------:    |  :------------------------: |
|   Age     |        Age of the patient             |       28 yrs - 77 yrs       |
|   Sex     |        Sex of the patient             |       Male, Female          |
|   exang   |        exercise induced angina        |       (1 = yes; 0 = no)     |
|   caa     |        number of major vessels        |       (0-3)                 |
|   cp      |        Chest Pain type chest pain type|       Value 1: typical angina   [TA] |
|           |                                       |       Value 2: atypical angina  [ATA]|
|           |                                       |       Value 3: non-anginal pain [NAP]|
|           |                                       |       Value 4: asymptomatic     [ASY]|
|   trtbps  |     resting blood pressure (in mm Hg) |       0 - 200 mm Hg                       |
|   chol    |   cholestoral in mg/dl fetched via BMI sensor |   0-603 mg/dl                     |
|   fbs     |        (fasting blood sugar > 120 mg/dl) |        (1 = true; 0 = false)      |   
|   rest_ecg|        resting electrocardiographic results   |  Value 0: normal              |
|           |                                       |          Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or  | |           |                                       |          depression of > 0.05 mV)                                                         |
|           |                                       |     Value 2: showing probable or definite left ventricular hypertrophy by Estes'  criteria |
|   thalach |       maximum heart rate achieved     |       60 - 202 bpm                              |
|   target  |        chance of a heart attack       |       0= less chance of heart attack; 1= more chance of heart attack |                                                           

------------------------------------------------------------------------------------------------------------------------------------------------

HEART2: https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction

# **Data Cleaning, Setup, & Exploration**

We did some feature engineering and created levels within the 'Age' variable: {[28-37], [38-47], [48-57], [58-67], [68-77]}
```{r setup}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(tidyverse, rstatix, class,
               rpart, rpart.plot, dplyr, corrplot, MASS, caret, MVN, 
               factoextra, psych)
source("Partial F Test function.R")

heart2 <- read.csv("heart2.csv")

# ----------------------- Data for Classifying Heart Disease -----------------------

heart <- heart2  %>% 
    mutate(Age = if_else(Age >= 28 & Age <= 37,
                         "28-37",
                         if_else(Age >= 38 & Age <= 47, 
                                 "38-47",
                         if_else(Age >= 48 & Age <= 57, 
                                 "48-57", 
                         if_else(Age >= 58 & Age <= 67, 
                                 "58-67",
                         if_else(Age >= 68 & Age <= 77, 
                                 "68-77", "Not seen"))))), 
           HeartDisease = if_else(HeartDisease == "0",
                       "Unaffected",
                       "Affected") %>% factor())

# ----------------------- Data for Classifying Chest Pain ---------------------- 

heart_CP <- heart2  %>% 
    mutate(Age = if_else(Age >= 28 & Age <= 37,
                         "28-37",
                         if_else(Age >= 38 & Age <= 47, 
                                 "38-47",
                         if_else(Age >= 48 & Age <= 57, 
                                 "48-57", 
                         if_else(Age >= 58 & Age <= 67, 
                                 "58-67",
                         if_else(Age >= 68 & Age <= 77, 
                                 "68-77", "Not seen"))))),
           ChestPainType = factor(ChestPainType,
                      levels = c("TA", "ATA", "NAP", "ASY")))

# Need to edit this?

N <- nrow(heart); p <- ncol(heart %>%dplyr::select(where(is.numeric)));

# number of groups in ChestPainType, k_pain.
k_pain <- n_distinct(heart$ChestPainType)
# number of groups in age, k_age
k_age <- n_distinct(heart$Age)
# number of groups in Heart Disease, k_HD
k_HD <- n_distinct(heart$HeartDisease)

# Combined 5 datasets (Cleveland, Long Beach, Switzerland, Hungarian, & Stalog)
skimr::skim(heart)

```

### Correlation Plot of Numeric Variables

```{r Exploratory}

R <- cor(heart %>%dplyr::select(where(is.numeric)))

corrplot(R, 
        method="shade", 
        type="upper", 
        addCoef.col = "yellow")

table(heart$Sex)

```
Variances of the numeric variables:
RestingBP - 342.7739
Cholesterol - 11964.89
MaxHR - 648.2286
Oldpeak - 1.137572
As shown by the correlation plot above of the numeric variables in our data set, there does not
appear to be any high correlations between variables.

### A Look at Chest Pain Type Boxplots  

```{r box}
heart %>% 
  pivot_longer(cols = c(RestingBP, Cholesterol, Oldpeak, MaxHR),
               names_to = "attribute",
               values_to = "value") %>% 
  
  ggplot(mapping = aes(x = value,
                       fill = ChestPainType)) + 
  geom_boxplot() + 
  facet_wrap(facets = ~ attribute,
             scales = "free") +
  labs(fill = "ChestPainType") +
  theme(legend.position = "top")
```

Above is a set of box plots showing the distribution of the four chest pain types in each of the
5 numeric variables in our data set. The chest pain types appear relatively equal across
Cholesterol and RestingBP, while MaxHR is noticeably lower for those with ASY, and both
ASY and TA are noticeably higher in Oldpeak.


### Density Plots of Heart Disease

```{r dens}
heart %>% 
  pivot_longer(cols = c(RestingBP, Cholesterol, Oldpeak, MaxHR),
               names_to = "attribute",
               values_to = "value") %>% 
  
  ggplot(mapping = aes(x = value,
                       fill = HeartDisease)) + 
  geom_density(alpha = .5) + 
  facet_wrap(facets = ~ attribute,
             scales = "free") +
  labs(fill = "HeartDisease") +
  theme(legend.position = "top")

```

Also shown above is a set of density plots showing the distribution of those affected or unaffected by
heart disease in each of the 5 numeric variables in our data set. It appears that MaxHR has
a higher median for those without heart disease when compared to those with heart disease.
It appears that Oldpeak and RestingBP have slightly higher medians for those with heart
disease when compared to those without heart disease. Cholesterol level appears to be
relatively equal between the two.

### Check some group means

```{r Group Means, warning=FALSE}

HD_means <-
  heart %>% 
  group_by(HeartDisease, Age) %>% 
  summarize(across(.cols = c(Cholesterol, Oldpeak, MaxHR),
                   .fns = mean))
view(HD_means)

```

### PCA to Check Significance of Variables 

```{r PCA}

(heart_R_PCA <- prcomp(heart %>% dplyr::select(where(is.numeric)),
                      scale. = T))
summary(heart_R_PCA)


fviz_screeplot(X = heart_R_PCA,
               choice = "eigenvalue",
               geom = "line",
               linecolor = "steelblue",
               ncp = p) + 
  
  labs(title = "Screeplot using the Covariance Matrix",
       x = "Principal Component") + 
  
  geom_hline(yintercept = 1,
             color = "darkred")


```

### Correlation Matrix PCA Biplot

```{r 2c_biplot}

fviz_pca(X = heart_R_PCA,
         axes = c(1, 2),              # Which PCs to plot
         geom = c("point"), 
         alpha.ind = .1, 
         repel = T)  +   # text adds name of country.

  coord_equal()


```

We used PCA to check variable dependencies, as well as significance of the variables. To
no surprise, PCA wasnâ€™t super useful because there wasnâ€™t much collinearity between the
numeric variables (as shown in the correlation matrix). This is shown in the screeplot,
because the first two PCâ€™s only account for around 55%, and â…˜ of the PCâ€™s would get us to
~88% of the proportion covered. The biplot also shows this because the direction of the
vectorâ€™s do not overlap - they point in mostly different directions.

### Check Differences Using MANOVA

We want to create a MANOVA model to check if there is a difference in mean chest pain type between predictor variables. Our null hypothesis is that there is no difference in mean chest pain type between any of the predictor variables while our alternative hypothesis is that there is a difference.

```{r Manova CHEST PAIN}


heart_man <- manova(cbind(RestingBP, Cholesterol, FastingBS, MaxHR, Oldpeak, HeartDisease) ~ ChestPainType,
    data = heart)

summary(heart_man)

```

Based on our test statistic which is very close to zero, we can conclude that there is a difference in mean chest pain type between at least one pair of predictor variables.

### Check Assumptions

```{r Assumptions}
# Not normal
mvn(data = heart_man$residuals, 
    desc = F, 
    multivariatePlot = "qq",
    univariateTest = "SW",
    mvnTest = "mardia")

box_m(data =  heart[, c(4, 5, 6, 8, 10)],
      group = heart$ChestPainType)

```
Checking assumptions:
The first assumption checked was to see if the data is multivariate normal. After performing a
test for mardia skewness and mardia kurtosis, it appears that the data is not multivariate
normal as the test for normality gave a p-value of 3.997e-82 for mardia skewness and a
p-value of 0 for mardia kurtosis. As shown by the QQ plot below, there is evidence of
skewness as well.

### Let's see what's actually useful:

```{r}

Partial_F(Y = heart_CP %>% 
              dplyr::select(FastingBS, RestingBP, Cholesterol, Oldpeak, MaxHR, HeartDisease), 
          x = heart_CP$ChestPainType)


# ------------------------------------------ # 
Partial_F(Y = heart_CP %>% 
              dplyr::select(RestingBP, Cholesterol, Oldpeak, MaxHR, HeartDisease), 
          x = heart_CP$ChestPainType)

# ------------------------------------------ # 

Partial_F(Y = heart_CP %>% 
              dplyr::select(Cholesterol, Oldpeak, MaxHR, HeartDisease), 
          x = heart_CP$ChestPainType)


# ------------------------------------------ # 



Partial_F(Y = heart_CP %>% 
              dplyr::select(Oldpeak, MaxHR, HeartDisease), 
          x = heart_CP$ChestPainType)


# ------------------------------------------ # 


Partial_F(Y = heart_CP %>% 
              dplyr::select(MaxHR, HeartDisease), 
          x = heart_CP$ChestPainType)

heart_man <- manova(cbind(MaxHR, HeartDisease) ~ ChestPainType,
    data = heart_CP)

summary(heart_man)


# ------------------------------------------ # 

# --------------CHECKING HEART DISEASE AS OUTCOME ------------------# 

# ------------------------------------------ # 
Partial_F(Y = heart %>% 
              dplyr::select(RestingBP, Cholesterol, Oldpeak, MaxHR), 
          x = heart$HeartDisease)


# ------------------------------------------ # 
Partial_F(Y = heart %>% 
              dplyr::select(Cholesterol, Oldpeak, MaxHR), 
          x = heart$HeartDisease)


# Stratify by ChestPain Type:
heart_man <- manova(cbind(Cholesterol, Oldpeak, MaxHR) ~ ChestPainType + HeartDisease,
    data = heart)

summary(heart_man)

#Age, Sex, RestingBP, Cholesterol, FastingBS, RestingECG, MaxHR, ExerciseAngina, Oldpeak, ST_Slope, HeartDisease
```
Through running partial f tests and removing our insignificant variables we found that Oldpeak, MaxHR, HeartDisease are the only variables we want to keep when predicting chest pain type. These are the only variables that contribute unique information and are important predictors. As for predicting heart disease, we found that MaxHR and Oldpeak were useful predictors with the addition of Cholesterol.


### Linear Discriminant Analysis
```{r Linear Discriminate}
# ------------ Plot the discriminant for HEART DISEASE ------------------

heart_HD_lda <- MASS::lda(HeartDisease ~ cbind(Cholesterol, Oldpeak, MaxHR),
                     data = heart)

ld_sep_pct <- round(heart_HD_lda$svd^2/sum(heart_HD_lda$svd^2)*100,
                    digits = 1)

heart_HD <- 
  data.frame(heart, 
             predict(heart_HD_lda)$x)

heart_HD_lda$scaling

gg_lda_density <-
  heart_HD %>%
  ggplot(mapping = aes(x = LD1,
                       fill = HeartDisease)) +

  theme(legend.position = "bottom") +

  labs(x = paste0("LD1 (Percent Explained: ", ld_sep_pct[1], "%)"),
       y = paste0("Density"))

gg_lda_density +
  geom_density(alpha = .5)

# ------------------ Plot Discrimant for CHEST PAIN -------------------- 
heart_CP_lda <- MASS::lda(ChestPainType ~ cbind(Oldpeak, MaxHR, HeartDisease),
                     data = heart_CP)

ld_sep_pct <- round(heart_CP_lda$svd^2/sum(heart_CP_lda$svd^2)*100,
                    digits = 1)

heart_CPLDA <-
  data.frame(heart_CP, 
             predict(heart_CP_lda)$x)


heart_CP_lda$scaling

gg_lda_scatter_CP <-
  heart_CPLDA %>%
  ggplot(mapping = aes(x = LD1,
                       y = LD2,
                       color = ChestPainType)) +

  theme(legend.position = "bottom") +

  labs(x = paste0("LD1 (Percent Explained: ", ld_sep_pct[1], "%)"),
       y = paste0("LD2 (Percent Explained: ", ld_sep_pct[2], "%)"))

gg_lda_scatter_CP +
  geom_point()

```

First, LDA was performed based on heart disease status and chest pain type. As shown in
the first graph below, the data is fairly well separated by the first linear discriminant based on
those affected or unaffected by heart disease. As shown in the second graph below, the four
types of chest pain are not very well separated by LD1 and LD2.


# **Predicting Heart Disease**

With our initial set up and data exploration complete, we are ready to move on to our methods. First, we decided to try to predict heart disease (affected or unaffected) using our set of predictor variables. We used QDA, KNN, and a classification tree to carry out these predictions.

### QDA: Predicting Heart Disease

```{r Assumptions HEART DISEASE}
# Using best model:
heart_man_HD <- manova(cbind(Cholesterol, Oldpeak, MaxHR) ~ HeartDisease,
    data = heart)


# Not normal
mvn(data = heart_man$residuals, 
    desc = F, 
    multivariatePlot = "qq",
    univariateTest = "SW",
    mvnTest = "mardia")

box_m(data =  heart[, c("Cholesterol", "Oldpeak", "MaxHR")],
      group = heart$HeartDisease)

```
To continue with discriminant analysis, a boxâ€™s m test was performed to test for equal
covariance matrices (as explained in the descriptive statistics section above). After rejecting
the null hypothesis, Quadratic Discriminant Analysis for both heart disease and chest pain
type was carried out.
```{r QDA Heart Disease}
# Not normal and reject box_m test: 
qda_heart_HD_cv <- MASS::qda(formula = HeartDisease~ cbind(Cholesterol, Oldpeak, MaxHR), 
                      data = heart, 
                      CV = T)

# Confusion Matrix
table(predicted = qda_heart_HD_cv$class, 
      actual = heart$HeartDisease) %>% 
  confusionMatrix()

```
When predicting heart disease, QDA performed fairly well,
achieving an accuracy score of about 76.8%.

```{r Standardize HD}
# Find the pooled standard deviations:
sd_heart_HD <- 
  summary(heart_man)$SS$Residuals %>%  
  diag() %>% 
  sqrt()/sqrt(N-k_HD)



# Standardize the data using the pooled standard deviations:


# Now we need to divide each variable by the pooled sd:
heart_sc_HD <- 
  scale(heart[, c("Cholesterol", "Oldpeak", "MaxHR")],
        center = T, 
        scale = sd_heart_HD) %>% 
  data.frame()


heart_sc_HD$HeartDisease <- heart$HeartDisease

```


### KNN Classification: Predicting Heart Disease

```{r 1eii_kChoice}
## Creating a loop to find the best choice for k
RNGversion("4.0.0")
set.seed(123)

# ---------------------- HEART DISEASE ----------------------- #
sqrt(N/k_HD)
k_choice <-5:55

# data.frame to store the predictions for different choices of k
knn_predictions <- data.frame(Actual = heart$HeartDisease)


# Function knn.cv() performs KNN using cross-validation
# and returns the predicted class based on the nearest neighbors.

# Looping through the different choices of k for knn
for (i in k_choice){
  
  knn_temp <- class::knn.cv(train = heart_sc_HD %>% dplyr::select(-HeartDisease), 
                            cl = heart_sc_HD$HeartDisease, 
                            k = i)
  
  # adding the predicted column to the data set
  knn_predictions <- 
    knn_predictions %>% 
    add_column(knn_temp)
}

# Changing the column names to better describe the results
colnames(knn_predictions) <- c('Actual', paste0("k", k_choice))

# Calculating the error rate for each choice of k:
knn_predictions %>% 
  pivot_longer(cols = starts_with("k"),
               names_to = "k_choice",
               values_to = "prediction") %>% 
  group_by(k_choice) %>% 
  summarize(incorrect = sum(Actual != prediction),
            positive_rate = mean(Actual == prediction)) %>% 
  mutate(k = parse_number(k_choice)) %>% 
  
  ggplot(mapping = aes(x = k,
                       y = positive_rate)) +
  geom_line(color = "darkred",
            size = 1) + 
  
  labs(x = "Choice of k",
       y = "Correct Prediction Percentage") +
  
  scale_x_continuous(breaks = k_choice) +
  scale_y_continuous(labels = scales::percent)


```


```{r 1eii_kNNCM}
# ---------------------- HEART DISEASE ---------------------- #

# Best choice of kNN model
heart_knn <- knn.cv(train = heart_sc_HD %>% dplyr::select(-HeartDisease), 
                          cl = heart_sc_HD$HeartDisease, 
                          k = 47)

# Confusion matrix 
data.frame(actual = heart$HeartDisease, 
           predicted = heart_knn) %>%
  table() %>%
  confusionMatrix()



```
The next algorithm used was KNN, where
the choices for k were looped through to find the ideal choice when carrying out the
algorithm. K = 47 was determined to be the best choice for predicting heart disease as it yielded the highest accuracy rate. The KNN algorithm performed
relatively well when predicting heart disease status with an accuracy score of 79.41%.


### Classification Tree: Predicting Heart Disease

``` {r HEART DISEASE Tree}

# Include the two lines below at the top of the R code to ensure your answer matches the solutions
RNGversion("4.0.0")
set.seed(123)


# Create the full classification tree
heart_tree2 <- rpart(HeartDisease ~ .- ChestPainType,
                   data = heart,
                   minsplit = 2,
                   minbucket = 1,
                   cp = -1,
                   method = "class")


# Looking at the cp table to find the optimal pruning value:
# simplest tree where xerror < min(xerror) + min(xstd)
printcp(heart_tree2)
plotcp(heart_tree2)



# Prune the tree

p_heart_tree2<- prune(heart_tree2, cp= 0.00731707)


# Plot the pruned tree

rpart.plot(p_heart_tree2,
           type=5,
           extra = 101)


# Display the confusion matrix
pheart_tree_pred2 <- predict(object = p_heart_tree2,
                         newdata = heart,
                         type = 'class')

data.frame(actual = heart$HeartDisease,
           predicted = pheart_tree_pred2) %>%
  table() %>%
  confusionMatrix()


```

The first step in creating a decision tree for predicting heart disease status was creating the
tree and determining the best complexity parameter (cp) for the final pruned tree. The ideal
value for cp was determined to be 0.00731707:

xerror < min(xerror) + min(xstd)

0.35122 < 0.33659 + 0.026411

0.35122 gives a CP value of 0.00731707

The output of the pruned tree is shown above This tree returned an accuracy score of
88.45%. ST_Slope was the first predictor variable considered. After this the tree considers all other predictor variables in its decisions besides age which is an interesting take away. This means that age on its own is not a very useful variable for predicting if someone has heart disease, according to this model. With about 88% accuracy this is by far our best model for predicting heart disease.

# **Predicting Chest Pain Type**

After predicting Heart disease with relatively high success, we decided to move on and attempt to predict chest pain type. We used the same three methods: QDA, KNN, and a classification tree.

### QDA: Predicting Chest Pain

```{r QDA CHEST PAIN}
# Not normal
qda_heart_CP_cv <- MASS::qda(formula = ChestPainType ~ cbind(Oldpeak, MaxHR, HeartDisease), 
                      data = heart_CP, 
                      CV = T)

# Confusion Matrix
table(predicted = qda_heart_CP_cv$class, 
      actual = heart_CP$ChestPainType) %>% 
  confusionMatrix()

```
When predicting chest pain type, QDA was not as effective as predicting heart
disease, as it achieved an accuracy score of 61.44%. As shown in the confusion
matrix, the most commonly misclassified chest pain types were NAP (52.615%
balanced accuracy) and TA (49.828% balanced accuracy).

```{r Standardize CHEST PAIN}
# Make Manova model:

heart_man_CP <- manova(cbind(Oldpeak, MaxHR, HeartDisease) ~ ChestPainType,
    data = heart_CP)

# Find the pooled standard deviations:
sd_heart_CP <- 
  summary(heart_man_CP)$SS$Residuals %>%  
  diag() %>% 
  sqrt()/sqrt(N-k_pain)



# Standardize the data using the pooled standard deviations:

typeof(heart_CP$HeartDisease)
# Now we need to divide each variable by the pooled sd:
heart_sc_CP <- 
  scale(heart_CP[, c("Oldpeak", "MaxHR", "HeartDisease")],
        center = T, 
        scale = sd_heart_CP) %>% 
  data.frame()

heart_sc_CP$ChestPainType <- heart_CP$ChestPainType

```


### KNN Classification: Predicting Chest Pain

```{r chest knn}
## Creating a loop to find the best choice for k
RNGversion("4.0.0")
set.seed(123)
# ---------------------- CHEST PAIN ---------------------- #
sqrt(N/k_pain)
k_choice <-5:27

# data.frame to store the predictions for different choices of k
knn_predictions <- data.frame(Actual = heart_CP$ChestPainType)


# Function knn.cv() performs KNN using cross-validation
# and returns the predicted class based on the nearest neighbors.

# Looping through the different choices of k for knn
for (i in k_choice){

  knn_temp <- class::knn.cv(train = heart_sc_CP %>% dplyr::select(-ChestPainType),
                            cl = heart_sc_CP$ChestPainType,
                            k = i)

  # adding the predicted column to the data set
  knn_predictions <-
    knn_predictions %>%
    add_column(knn_temp)
}

# Changing the column names to better describe the results
colnames(knn_predictions) <- c('Actual', paste0("k", k_choice))

# Calculating the error rate for each choice of k:
knn_predictions %>%
  pivot_longer(cols = starts_with("k"),
               names_to = "k_choice",
               values_to = "prediction") %>%
  group_by(k_choice) %>%
  summarize(incorrect = sum(Actual != prediction),
            positive_rate = mean(Actual == prediction)) %>%
  mutate(k = parse_number(k_choice)) %>%

  ggplot(mapping = aes(x = k,
                       y = positive_rate)) +
  geom_line(color = "darkred",
            size = 1) +

  labs(x = "Choice of k",
       y = "Correct Prediction Percentage") +

  scale_x_continuous(breaks = k_choice) +
  scale_y_continuous(labels = scales::percent)

# ---------------------- CHEST PAIN ---------------------- # 
# # Best choice of kNN model
heart_knn <- knn.cv(train = heart_sc_CP%>% dplyr::select(-ChestPainType),
                          cl = heart_sc_CP$ChestPainType,
                          k = 16)

# Confusion matrix
data.frame(actual = heart_CP$ChestPainType,
           predicted = heart_knn) %>%
  table() %>%
  confusionMatrix()

```

When predicting chest pain type, the choices for k were looped through to find the ideal
choice when carrying out the algorithm. K = 16 was determined to be the best choice as it
yielded the highest accuracy rate. The KNN algorithm performed fairly poorly when predicting chest pain status with an accuracy score of 62.31%.


### Classification Tree: Predicting Chest Pain

```{r Tree CHEST PAIN, message=FALSE}
# Include the two lines below at the top of the R code to ensure your answer matches the solutions
RNGversion("4.0.0")
set.seed(123)
typeof(heart_CP$HeartDisease)

# Create the full classification tree
heart_tree_CP <- rpart(ChestPainType ~ .-HeartDisease, 
                   data = heart_CP, 
                   minsplit = 2, 
                   minbucket = 1, 
                   cp = -1,
                   method = "class")


# Looking at the cp table to find the optimal pruning value:
# simplest tree where xerror < min(xerror) + min(xstd)
printcp(heart_tree_CP)
plotcp(heart_tree_CP)

```


```{r CHEST PAIN Prune Tree}
# Prune the tree

p_heart_tree_CP<- prune(heart_tree_CP, cp= 0.02132701)


# Plot the pruned tree

rpart.plot(p_heart_tree_CP, 
           type=5, 
           extra = 101)


# Display the confusion matrix
pheart_tree_pred <- predict(object = p_heart_tree_CP,
                         newdata = heart_CP, 
                         type = 'class')

data.frame(actual = heart_CP$ChestPainType, 
           predicted = pheart_tree_pred) %>%
  table() %>%
  confusionMatrix()

```
The ideal value for cp for a decision tree for chest pain was determined to be 0.02132701:

xerror < min(xerror) + min(xstd)

0.87204 < 0.84597 + 0.035001

0.87204 gives a CP value of 0.02132701

The output of the pruned tree is shown above. This tree returned an accuracy score of
61.44%. Excercise angina was the first factor considered in the tree with those affected
classified as having ASY chest pain. For those with no exercise angina, people with an ST slope of Down or flat were classified as having ASY chest pain as well. Next, those with cholesterol under 131 were also classified as having ASY chest pain. People with cholesterol over or equal to 131 were than either classified as having ATA chest pain (in ages 28 to 57) or NAP chest pain (in age groups 58 to 77). 

# Factor Analysis

```{r Checking Factor Analysis}
# Using correlation matrix to check if factor analysis would be worth it:
KMO(R)



```
Since none of the values are greater than .55 Kaiser-Meyer-Olkin (KMO) index, Kasier suggests that our data is "miserable" for Factor Analysis.


# **Conclusion**

Our goal was to use patient health data to predict whether someone has heart
disease as well as what kind of chest pain they are likely to have. With cardiovascular illness
related deaths so prevalent in the United States, it is vital that work is done to catch heart
disease and chest pain in patients before it is too late. This project provides useful insight
into what the most significant indicators of heart disease and chest pain are and
simultaneously allows us to see what preventative measures can be taken to reduce risk of
heart disease. We had success in meeting our research objectives, most notably in
predicting heart disease status. Our best model was the decision tree which had an
accuracy of about 88.45% in predicting heart disease status. This means that given data of a
new patient in the same format as used in the model, we have around an 88.45% chance of
correctly predicting whether or not they have heart disease. We did not have much success
with predicting heart pain type and only achieved an accuracy of around 62% with our best
model. KNN was very marginally better than our decision tree with an accuracy of 62.31%
versus 61.44%. Because this difference is so small and decision trees are more easily
interpretable, we concluded that the decision tree is the best method for predicting chest
pain type. Overall, the decision tree method proved to be the most accurate and
interpretable out of all three methods we attempted using.

# **Limitations and Recommendations**

One limitation we encountered in our data set was that the data were not multivariate
normal (MVN). When conducting mardiaâ€™s test for MVN we found very strong evidence in
favor of rejecting the null hypothesis that the data are MVN. Mardiaâ€™s tests for skewness and
kurtosis yielded p-values close to zero giving us this evidence. Additionally, our chi-square
QQ-plot indicates that the data are not MVN. In this plot there is a significant portion of
observations whose squared Mahalanobis distances are much greater than their chi-square
quantile values. This leads to a deviation from a straight line in the plot indicating
non-normality. Luckily, multivariate normality is not required for QDA, although we could
have had an even more accurate model if it was present. KNN and the classification tree are
non-parametric and therefore by definition do not require multivariate normality.

As for the data itself, we can predict whether a patient has heart disease and what
type of chest pain they have fairly well, but we do not have the full picture in terms of the
patients profiles. In a perfect world the data would include more descriptive statistics
including diet, exercise, smoking habits, drinking habits, etc. With these other variables we
would be able to see what habits contribute to chest pain and heart disease in addition to
cholesterol, resting blood pressure, resting ecg etc. In terms of drawing conclusions, we do
not have any data about race, ethnicity, or comorbidities. We are unable to see how heart
disease and chest pain differs between these groups of people and therefore miss out on
being able to make predictions specific to particular groups. Also, there is a major class
imbalance within the sex variable. There are 725 males and only 193 females in the data set
making our findings heavily influenced by data about men.